{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping using huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "# INPUT TWEET IS ALREADY NORMALIZED!\n",
    "line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer(line, padding=\"max_length\", max_length=64, truncation=True).input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of tokens in the input\n",
    "print(input_ids.shape)\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bertweet(input_ids)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained architecture\n",
    "print(repr(bertweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model from the pretrained architecture with module\n",
    "class BERTweetClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BERTweetClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(64*768, 2)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        features = self.bert(input_ids)[0]\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        output = self.dropout(features)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = BERTweetClassifier(freeze_bert=False)\n",
    "print(repr(bert_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert_classifier(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with data from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.loc[0, \"text\"])\n",
    "input_ids = torch.tensor([tokenizer.encode(train_df.loc[0, \"text\"])])\n",
    "text_decoded = tokenizer.decode(input_ids[0])\n",
    "print(text_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a tensor of all input ids with padding from tokenizer\n",
    "encoded_input = tokenizer(list(train_df.text), add_special_tokens=True, padding=True, truncation=True)\n",
    "input_ids = torch.tensor(encoded_input['input_ids'])\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Val, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train validaion and test\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation dataset and dataloader\n",
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_input = tokenizer(list(df.text), add_special_tokens=True, padding=\"max_length\", max_length=64, truncation=True)\n",
    "        self.input_ids = torch.tensor(self.encoded_input['input_ids'])\n",
    "        self.labels = torch.tensor(df.target.values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = DisasterTweetsDataset(train_df, tokenizer)\n",
    "val_dataset = DisasterTweetsDataset(val_df, tokenizer)\n",
    "test_datset = DisasterTweetsDataset(test_df, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_datset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch in train_dataloader:\n",
    "    inputs, labels = batch\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop for the model\n",
    "def train(model, train_dataloader, val_dataloader, epochs=10, lr=1e-5, device='cpu'):\n",
    "    # Define loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Define scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    # Define metrics\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "    for epoch, i in enumerate(pbar):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for batch in train_dataloader:\n",
    "            # Get data\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids) # (batch_size, 2)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(preds.float(), labels.float())\n",
    "            loss.requires_grad = True\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_correct += torch.sum(preds == labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss and accuracy\n",
    "        avg_train_loss = (total_loss / len(train_dataloader))\n",
    "        avg_train_acc = (total_correct.double() / len(train_dataloader)).item()\n",
    "        metrics['train_loss'].append(avg_train_loss)\n",
    "        metrics['train_acc'].append(avg_train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for batch in val_dataloader:\n",
    "            # Get data\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            loss = loss_fn(preds.float(), labels.float()) \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_correct += torch.sum(preds == labels)\n",
    "\n",
    "        # Calculate average loss and accuracy\n",
    "        avg_val_loss = (total_loss / len(val_dataloader))\n",
    "        avg_val_acc = (total_correct.double() / len(val_dataloader)).item()\n",
    "        metrics['val_loss'].append(avg_val_loss)\n",
    "        metrics['val_acc'].append(avg_val_acc)\n",
    "\n",
    "        # Update progress bar with val accuracy and train accuracy\n",
    "        pbar.set_postfix({'train_acc': avg_train_acc, 'val_acc': avg_val_acc})\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['train_loss'], label='train')\n",
    "    plt.plot(metrics['val_loss'], label='val')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['train_acc'], label='train')\n",
    "    plt.plot(metrics['val_acc'], label='val')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add progress bar, make sure what is logged in mettric is float and not tensor\n",
    "# test loop for the model\n",
    "def test(model, test_dataloader, device='cpu'):\n",
    "\n",
    "    # Define loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define metrics\n",
    "    metrics = {\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in tqdm(range(test_dataloader), desc=\"Testing\", position=0, leave=True):\n",
    "        # Get data\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        loss = loss_fn(preds.float(), labels.float()) # outputs[0] is the logits\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs[0], dim=1)\n",
    "        total_correct += torch.sum(preds == labels)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_test_loss = total_loss / len(test_dataloader)\n",
    "    avg_test_acc = (total_correct.double() / len(test_dataloader)).item()\n",
    "    metrics['test_loss'].append(avg_test_loss)\n",
    "    metrics['test_acc'].append(avg_test_acc)\n",
    "    print(f\"Test loss {avg_test_loss} accuracy {avg_test_acc}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "bert_classifier = BERTweetClassifier(freeze_bert=False)\n",
    "\n",
    "# Select device based on availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device used : {device}\")\n",
    "bert_classifier.to(device)\n",
    "\n",
    "# train the model\n",
    "metrics, model = train(bert_classifier, train_dataloader, val_dataloader, epochs=10, lr=1e-5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "test_metrics = test(model, test_dataloader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f0a451b91d21e89423ec86b823c013ebf520754bdcd7b6442031c0152fc2aaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
