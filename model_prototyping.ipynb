{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Prototyping"
   ],
   "metadata": {
    "id": "ocn9j56OC-tJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "id": "8Tsi6DtVC-tO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install textacy\n",
    "!pip install wandb\n",
    "!pip install optuna"
   ],
   "metadata": {
    "id": "y6fXf54LSND0",
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-12-06T15:40:17.464909Z",
     "iopub.execute_input": "2022-12-06T15:40:17.465590Z",
     "iopub.status.idle": "2022-12-06T15:41:10.352111Z",
     "shell.execute_reply.started": "2022-12-06T15:40:17.465554Z",
     "shell.execute_reply": "2022-12-06T15:41:10.350915Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mCollecting textacy\n  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.4/200.4 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hCollecting jellyfish>=0.8.0\n  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.6/132.6 kB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.0.2)\nRequirement already satisfied: cytoolz>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from textacy) (0.12.0)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (2.5)\nRequirement already satisfied: joblib>=0.13.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.0.1)\nCollecting pyphen>=0.10.0\n  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m42.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: spacy>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (3.3.1)\nRequirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (2.28.1)\nRequirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.7/site-packages (from textacy) (4.64.0)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.7.3)\nRequirement already satisfied: cachetools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (4.2.4)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->textacy) (5.1.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.26.12)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (8.0.17)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.10.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.9)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.8)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (21.3)\nRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (4.1.1)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.7.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.1.2)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.6.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.7)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.4.2)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.8.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.8)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.10)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.4.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (59.8.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy>=3.0.0->textacy) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (4.13.0)\nBuilding wheels for collected packages: jellyfish\n  Building wheel for jellyfish (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=102031 sha256=9d94629797159d8de84ca880555d8ae02d31e2eb8b7331a26d03d5b28e9911a2\n  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\nSuccessfully built jellyfish\nInstalling collected packages: pyphen, jellyfish, textacy\nSuccessfully installed jellyfish-0.9.0 pyphen-0.13.2 textacy-0.11.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.9.10)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.13.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mRequirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (3.0.3)\nRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.7.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (6.0)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.10.1)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.39)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.21.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.64.0)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.8.2)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.8.1)\nRequirement already satisfied: importlib-metadata<5.0.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (4.13.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (6.7.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (21.3)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (5.8.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (1.2.3)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (3.0.9)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.2)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.3.0)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.5.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.10.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.4.2)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (0.5.1)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Hyperparameters optimization\n",
    "import optuna\n",
    "import wandb\n",
    "import joblib"
   ],
   "metadata": {
    "id": "7rXI-gSqC-tP",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:37:54.559648Z",
     "iopub.execute_input": "2022-12-06T15:37:54.560195Z",
     "iopub.status.idle": "2022-12-06T15:38:02.143875Z",
     "shell.execute_reply.started": "2022-12-06T15:37:54.560147Z",
     "shell.execute_reply": "2022-12-06T15:38:02.142759Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Google drive files for data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# GPU used \n",
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "g9OvryARNRY-",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:33:22.376389Z",
     "iopub.execute_input": "2022-12-06T15:33:22.377264Z",
     "iopub.status.idle": "2022-12-06T15:33:23.532317Z",
     "shell.execute_reply.started": "2022-12-06T15:33:22.377157Z",
     "shell.execute_reply": "2022-12-06T15:33:23.531082Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Tue Dec  6 15:33:23 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prototyping using huggingface transformers"
   ],
   "metadata": {
    "id": "RJXsBwFpC-tR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "# INPUT TWEET IS ALREADY NORMALIZED!\n",
    "line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer(line, padding=\"max_length\", max_length=64, truncation=True).input_ids])\n"
   ],
   "metadata": {
    "id": "adm6JpARC-tS",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:41:10.354456Z",
     "iopub.execute_input": "2022-12-06T15:41:10.356214Z",
     "iopub.status.idle": "2022-12-06T15:41:12.161643Z",
     "shell.execute_reply.started": "2022-12-06T15:41:10.356171Z",
     "shell.execute_reply": "2022-12-06T15:41:12.160670Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9702925c9e6428991f4b49ee3ef010b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c5d4ec397d34d10974239bed66d2ce0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0fed2f6952b4b64870e0c7107fd9f1e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check for how many tokens we'll need"
   ],
   "metadata": {
    "id": "XsThsTjbN0Yc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"/kaggle/input/train-disaster-tweets/train.csv\")"
   ],
   "metadata": {
    "id": "0NELkE7DOQBK",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:41:30.170295Z",
     "iopub.execute_input": "2022-12-06T15:41:30.170873Z",
     "iopub.status.idle": "2022-12-06T15:41:30.206119Z",
     "shell.execute_reply.started": "2022-12-06T15:41:30.170828Z",
     "shell.execute_reply": "2022-12-06T15:41:30.205077Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max = 0 \n",
    "for text in data['text']:\n",
    "  input_ids = tokenizer(text).input_ids\n",
    "  max = np.max((max, len(input_ids)))"
   ],
   "metadata": {
    "id": "GVVnNv55OJtz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(max)"
   ],
   "metadata": {
    "id": "SartX8ySP8BQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the maximum number of tokens is 64, to take into account the fact that tweets from the test set may be longer, we'll set the max number of tokens to 90, this will speed the training."
   ],
   "metadata": {
    "id": "Wv1qTacAQCXz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output = tokenizer.encode(line)\n",
    "print(output)"
   ],
   "metadata": {
    "id": "c9ACBchSLttN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_dict = tokenizer.encode_plus(line, add_special_tokens=True, max_length=90, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "input_ids = output_dict.input_ids\n",
    "attention_mask = output_dict.attention_mask"
   ],
   "metadata": {
    "id": "sTlVx4NjSEjB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids"
   ],
   "metadata": {
    "id": "H8KzTZq8TRR_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "attention_mask"
   ],
   "metadata": {
    "id": "44kalQSPTShW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data prep"
   ],
   "metadata": {
    "id": "mMREaLKpRvmN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "# define clean function\n",
    "# add / remove any line if necessary\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; by their plain-text representation\n",
    "    text = html.unescape(text) \n",
    "    \n",
    "    # subsitute tags like <tab> by spaces in the specified text or remove them\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    \n",
    "    # subsitute markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    \n",
    "    # subsitute text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    \n",
    "    # subsitute standalone sequences of specials, matches &# but NOT #hashtag\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "\n",
    "     # subsitute standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    \n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    #remove stock market tickers like $GE\n",
    "    text = re.sub(r'\\$\\w*', '', text)  \n",
    "    \n",
    "    #remove old style retweet text \"RT\"\n",
    "    text = re.sub(r'RT[\\s]+', '', text)        \n",
    "    text = re.sub(r'DT[\\s]+', '', text)   \n",
    "    \n",
    "    #remove hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    return text.strip()"
   ],
   "metadata": {
    "id": "RNaQUl6MRygi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from textacy import preprocessing\n",
    "from functools import partial\n",
    "\n",
    "# create cleaning pipeline\n",
    "preproc = preprocessing.make_pipeline(\n",
    "    \n",
    "    # join words split by a hyphen or line break\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    \n",
    "    # subsitute fancy quatation marks with an ASCII equivalent\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    \n",
    "    # normalize unicode characters in text into canonical forms\n",
    "    preprocessing.normalize.unicode,\n",
    "    \n",
    "    # remove any accents character in text by replacing them with ASCII equivalents or removing them entirely\n",
    "    preprocessing.remove.accents,\n",
    "\n",
    "    # remove all email addresses in text \n",
    "    partial(preprocessing.replace.emails, repl= \"\"), # or _EMAIL_\n",
    "    \n",
    "    # remove all phone numbers in text \n",
    "    partial(preprocessing.replace.phone_numbers, repl=\"\"), # or _PhoneNumber_\n",
    "    \n",
    "    # remove all URLs in text \n",
    "    partial(preprocessing.replace.urls, repl= \"\"), # or _URL_\n",
    "    \n",
    "    # remove all (Twitter-style) user handles in text \n",
    "    partial(preprocessing.replace.user_handles, repl=\"\"), # or _HANDLE_\n",
    "    \n",
    "    # Replace all hashtags in text with repl.\n",
    "    #partial(preprocessing.replace.hashtags, repl=\"_HASTAG_\"),\n",
    "    \n",
    "    ### TEST ### Enable it only before generating tokens for word clouds\n",
    "    partial(preprocessing.replace.numbers, repl=\"\"),\n",
    "    \n",
    "    # remove HTML tags from text\n",
    "    preprocessing.remove.html_tags,\n",
    "\n",
    "    # remove text within curly {}, square [], and/or round () brackets\n",
    "    preprocessing.remove.brackets,\n",
    "\n",
    "    # replace specific set of punctuation marks with whitespace\n",
    "    partial(preprocessing.remove.punctuation, only=[ \",\", \":\", \";\", \"/\", \" \",\"(\",\"@\"]),\n",
    "    \n",
    "    # Replace all currency symbols in text with repl\n",
    "    preprocessing.replace.currency_symbols,\n",
    "    \n",
    "    # replace all emoji and pictographs in text with repl.\n",
    "    preprocessing.replace.emojis,\n",
    "    \n",
    " )"
   ],
   "metadata": {
    "id": "BMVlh7dFR32D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "data['text_c'] = data['text'].apply(clean)\n",
    "data['text_clean'] = data['text_c'].apply(preproc)\n",
    "data['text_clean'] = data['text_clean'].str.lower()"
   ],
   "metadata": {
    "id": "BAMccJNkSAU1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "tweets = data['text'].to_list()\n",
    "\n",
    "for tweet in tweets:\n",
    "  output_dict = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=90, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "  input_ids.append(output_dict.input_ids)\n",
    "  attention_masks.append(output_dict.attention_mask)\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "labels = torch.tensor(data['target'].values)"
   ],
   "metadata": {
    "id": "9nf85aqHUwvH",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:41:56.074818Z",
     "iopub.execute_input": "2022-12-06T15:41:56.075218Z",
     "iopub.status.idle": "2022-12-06T15:42:00.336337Z",
     "shell.execute_reply.started": "2022-12-06T15:41:56.075185Z",
     "shell.execute_reply": "2022-12-06T15:42:00.335122Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ],
   "metadata": {
    "id": "73yv1aTKDx_A",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:42:00.338293Z",
     "iopub.execute_input": "2022-12-06T15:42:00.338751Z",
     "iopub.status.idle": "2022-12-06T15:42:00.344582Z",
     "shell.execute_reply.started": "2022-12-06T15:42:00.338699Z",
     "shell.execute_reply": "2022-12-06T15:42:00.343280Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "train_size = int(0.85 * len(dataset)) # 85 - 15 % split\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32 # Bigger values will increase the gradient precision, lower values will reduce the memory load\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)"
   ],
   "metadata": {
    "id": "3cb3C8CzUJnx",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:42:00.579857Z",
     "iopub.execute_input": "2022-12-06T15:42:00.580568Z",
     "iopub.status.idle": "2022-12-06T15:42:00.587841Z",
     "shell.execute_reply.started": "2022-12-06T15:42:00.580530Z",
     "shell.execute_reply": "2022-12-06T15:42:00.586620Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training on all samples\n",
    "train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)"
   ],
   "metadata": {
    "id": "nmPzrCucD0ly"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters optimization"
   ],
   "metadata": {
    "id": "p9XnBWIutyx4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we are going to find the hyperparameters which reduce the val loss the most"
   ],
   "metadata": {
    "id": "h8oSjNZryDs0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define model inside this function, the archicture can be optimized with optuna\n",
    "def define_model(trial):\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels = 2, output_attentions = False, output_hidden_states = False)  \n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "id": "lO73MwNPtyDa",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:54:59.995398Z",
     "iopub.execute_input": "2022-12-06T15:54:59.995764Z",
     "iopub.status.idle": "2022-12-06T15:55:00.001537Z",
     "shell.execute_reply.started": "2022-12-06T15:54:59.995732Z",
     "shell.execute_reply": "2022-12-06T15:55:00.000309Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model_optuna(model, optimizer, scheduler, train_dataloader, trial, device = 'cpu', val_dataloader = None, epochs = 10):\n",
    "  pbar = tqdm(range(epochs))\n",
    "\n",
    "  metrics = {\n",
    "    \"epochs\" : [],\n",
    "    \"train_losses\" : [],\n",
    "    \"val_losses\" : [],\n",
    "    \"val_accs\" : [],\n",
    "    \"lr\" : [],\n",
    "  }\n",
    "\n",
    "  for i, epoch in enumerate(pbar):\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "      input_ids = batch[0].to(device)\n",
    "      input_masks = batch[1].to(device)\n",
    "      labels = batch[2].to(device)\n",
    "\n",
    "      model.zero_grad()\n",
    "\n",
    "      result = model(input_ids, token_type_ids = None, attention_mask = input_masks, labels = labels, return_dict = True)\n",
    "\n",
    "      loss = result.loss\n",
    "      logits = result.logits\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      # Call to scheduler only if it's linear\n",
    "      scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    total_val_acc = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    if val_dataloader is not None:\n",
    "\n",
    "      model.eval()\n",
    "      for i, batch in enumerate(val_dataloader):\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "          result = model(input_ids, token_type_ids = None, attention_mask = input_masks, labels = labels, return_dict = True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits.detach().cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        preds = np.argmax(logits, axis = 1).flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        val_acc = np.sum(preds == labels) / len(labels)\n",
    "\n",
    "        total_val_acc += val_acc\n",
    "        total_val_loss += loss.item()\n",
    "    \n",
    "      avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "      avg_val_acc = total_val_acc / len(val_dataloader)\n",
    "\n",
    "    metrics['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # Call to scheduler only if it's reduce on plateau\n",
    "    # scheduler.step(avg_val_loss)\n",
    "\n",
    "    metrics['epochs'].append(epoch)\n",
    "    metrics['train_losses'].append(avg_train_loss)\n",
    "\n",
    "    if val_dataloader is not None :\n",
    "      metrics['val_losses'].append(avg_val_loss)\n",
    "      metrics['val_accs'].append(avg_val_acc)\n",
    "      pbar.set_postfix({'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'val_acc' : avg_val_acc})\n",
    "\n",
    "    if trial is not None:\n",
    "      trial.report(avg_val_loss, epoch)\n",
    "\n",
    "      # Report metrics to wandb\n",
    "      wandb.log(data = {\n",
    "          'train_loss' : metrics['train_losses'][-1],\n",
    "          'val_loss' : metrics['val_losses'][-1],\n",
    "          'val_acc' : metrics['val_accs'][-1],\n",
    "          'lr' : metrics['lr'][-1]\n",
    "      }, step = epoch)\n",
    "\n",
    "      if trial.should_prune():\n",
    "        wandb.run.summary['state'] = \"pruned\"\n",
    "        wandb.finish(quiet = True)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "      # Saving the fine tunned model\n",
    "      ckpt = {\n",
    "          'epoch' : epoch,\n",
    "          'model_state' : model.state_dict(),\n",
    "          'optimizer_state' : optimizer.state_dict()\n",
    "      }\n",
    "\n",
    "  return model, metrics"
   ],
   "metadata": {
    "id": "-A9Xg7x91ZbM",
    "execution": {
     "iopub.status.busy": "2022-12-06T15:55:00.302285Z",
     "iopub.execute_input": "2022-12-06T15:55:00.302600Z",
     "iopub.status.idle": "2022-12-06T15:55:00.593376Z",
     "shell.execute_reply.started": "2022-12-06T15:55:00.302572Z",
     "shell.execute_reply": "2022-12-06T15:55:00.592351Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device : {device}\")\n",
    "\n",
    "def objective(trial):\n",
    "  \n",
    "    # Generate the model\n",
    "    model = define_model(trial).to(device) \n",
    "\n",
    "    # Define learning components (to be used in learning function)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-8, 5e-5)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr, betas = (0.9, 0.999), eps = 1e-8)\n",
    "\n",
    "    epochs = trial.suggest_int(\"epoch\", 1, 5)\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    num_warmup_steps = trial.suggest_int(\"warmup_steps\", 0, int(0.1 * total_steps))\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = total_steps)\n",
    "    \n",
    "    # init tracking with wandb\n",
    "    config = dict(trial.params)\n",
    "    config['trial.number'] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"twitter_disaster_tweets\",\n",
    "        entity=\"saulofein\",\n",
    "        config=config,\n",
    "        group=tag,\n",
    "        reinit=True        \n",
    "    )\n",
    "    \n",
    "    # Learning\n",
    "    model, metrics = train_model_optuna(model, optimizer, scheduler, train_dataloader, trial, val_dataloader = val_dataloader, device = device, epochs = epochs)\n",
    "    \n",
    "    # Compute the metrics Optuna will try to otpimize (maximize or minimize)\n",
    "    val_loss = metrics['val_losses'][-1]\n",
    "    \n",
    "    # Report the RMSE to wandb\n",
    "    wandb.run.summary['val_loss'] = val_loss\n",
    "    wandb.run.summary['state'] = 'completed'\n",
    "    wandb.finish(quiet = True)\n",
    "    \n",
    "    return val_loss"
   ],
   "metadata": {
    "id": "EAxjnh-KxZVH",
    "execution": {
     "iopub.status.busy": "2022-12-06T16:03:39.683163Z",
     "iopub.execute_input": "2022-12-06T16:03:39.683553Z",
     "iopub.status.idle": "2022-12-06T16:03:39.700593Z",
     "shell.execute_reply.started": "2022-12-06T16:03:39.683520Z",
     "shell.execute_reply": "2022-12-06T16:03:39.699411Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": "Using device : cuda\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tag = 'bert_1'\n",
    "\n",
    "study = optuna.create_study(direction = \"minimize\", study_name = tag)\n",
    "study.optimize(objective, n_trials = 100)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best trial :\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Value :\", best_trial.value)\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key} : {value}\")\n",
    "\n",
    "# Importance of hyperparameters\n",
    "importance_dict = optuna.importance.get_param_importances(study = study)\n",
    "plt.figure(figsize = (20, 15))\n",
    "sns.barplot(x = list(importance_dict.values()), y = list(importance_dict.keys()))\n",
    "plt.savefig(os.path.join(\"..\", \"hyperparameters_importance_{}.png\".format(tag.split('_')[1])))\n",
    "\n",
    "# Save best parameters dict\n",
    "joblib.dump(best_trial.params, f\"best_params_{tag}.pkl\")"
   ],
   "metadata": {
    "id": "IhIWHLOdxsLi",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best trial :\n",
    "  * Value : 0.3834475175374084\n",
    "  * Params:\n",
    "    * lr : 3.588461431799714e-05\n",
    "    * epoch : 1\n",
    "    * warmup_steps : 8"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Relative importance of hyperparameters :\n",
    "* Number of epochs : 70%\n",
    "* Learning rate : 30%"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "id": "4bcyjZJ3kazZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels = 2, output_attentions = False, output_hidden_states = False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "id": "LvqK2hOeZlK-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-6, betas = (0.9, 0.999), eps = 1e-8)"
   ],
   "metadata": {
    "id": "oB0OA1cxbUZh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_dataloader, val_dataloader = None, epochs = 10):\n",
    "  pbar = tqdm(range(epochs))\n",
    "\n",
    "  metrics = {\n",
    "    \"epochs\" : [],\n",
    "    \"train_losses\" : [],\n",
    "    \"val_losses\" : [],\n",
    "    \"val_accs\" : [],\n",
    "    \"lr\" : [],\n",
    "  }\n",
    "\n",
    "  for i, epoch in enumerate(pbar):\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "      input_ids = batch[0].to(device)\n",
    "      input_masks = batch[1].to(device)\n",
    "      labels = batch[2].to(device)\n",
    "\n",
    "      model.zero_grad()\n",
    "\n",
    "      result = model(input_ids, token_type_ids = None, attention_mask = input_masks, labels = labels, return_dict = True)\n",
    "\n",
    "      loss = result.loss\n",
    "      logits = result.logits\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      # Call to scheduler only if it's linear\n",
    "      scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    total_val_acc = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    if val_dataloader is not None:\n",
    "\n",
    "      model.eval()\n",
    "      for i, batch in enumerate(val_dataloader):\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "          result = model(input_ids, token_type_ids = None, attention_mask = input_masks, labels = labels, return_dict = True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits.detach().cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        preds = np.argmax(logits, axis = 1).flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        val_acc = np.sum(preds == labels) / len(labels)\n",
    "\n",
    "        total_val_acc += val_acc\n",
    "        total_val_loss += loss.item()\n",
    "    \n",
    "      avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "      avg_val_acc = total_val_acc / len(val_dataloader)\n",
    "\n",
    "    metrics['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # Call to scheduler only if it's reduce on plateau\n",
    "    # scheduler.step(avg_val_loss)\n",
    "\n",
    "    metrics['epochs'].append(epoch)\n",
    "    metrics['train_losses'].append(avg_train_loss)\n",
    "\n",
    "    if val_dataloader is not None :\n",
    "      metrics['val_losses'].append(avg_val_loss)\n",
    "      metrics['val_accs'].append(avg_val_acc)\n",
    "      pbar.set_postfix({'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'val_acc' : avg_val_acc})\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "      # Saving the fine tunned model\n",
    "      ckpt = {\n",
    "          'epoch' : epoch,\n",
    "          'model_state' : model.state_dict(),\n",
    "          'optimizer_state' : optimizer.state_dict()\n",
    "      }\n",
    "      torch.save(ckpt, \"/content/drive/MyDrive/Machine Learning/Models/weights_bertweet_intermediate.pt\")\n",
    "\n",
    "  # Saving the fine tunned model\n",
    "  ckpt = {\n",
    "          'epoch' : epoch,\n",
    "          'model_state' : model.state_dict(),\n",
    "          'optimizer_state' : optimizer.state_dict()\n",
    "      }\n",
    "  torch.save(ckpt, \"/content/drive/MyDrive/Machine Learning/Models/weights_bertweet_final.pt\")\n",
    "\n",
    "  return model, metrics"
   ],
   "metadata": {
    "id": "1ppga7Vybx2y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode = 'min', patience = 5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "model, metrics = train_model(model, optimizer, scheduler, train_dataloader, epochs = epochs)"
   ],
   "metadata": {
    "id": "kgaXDCEKH7_Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "\n",
    "plt.plot(metrics['train_losses'], label='train loss')\n",
    "plt.plot(metrics['val_losses'], label='val loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(metrics['val_accs'], label='val acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(metrics['lr'], label='lr')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.title('Learning rate')"
   ],
   "metadata": {
    "id": "p5TRgAyukeid"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_confusion_heatmap(\n",
    "        cf,\n",
    "        group_names=None,\n",
    "        categories=\"auto\",\n",
    "        count=True,\n",
    "        percent=True,\n",
    "        cbar=True,\n",
    "        xyticks=True,\n",
    "        xyplotlabels=True,\n",
    "        sum_stats=True,\n",
    "        figsize=None,\n",
    "        cmap=\"Blues\",\n",
    "        title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    \"\"\"\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = [\"\" for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names) == cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\n",
    "            \"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)\n",
    "        ]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [\n",
    "        f\"{v1}{v2}{v3}\".strip()\n",
    "        for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)\n",
    "    ]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        # Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        # if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf) == 2:\n",
    "            # Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1, 1] / sum(cf[:, 1])\n",
    "            recall = cf[1, 1] / sum(cf[1, :])\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy, precision, recall, f1_score\n",
    "            )\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize == None:\n",
    "        # Get default figure size if not set\n",
    "        figsize = plt.rcParams.get(\"figure.figsize\")\n",
    "\n",
    "    if xyticks == False:\n",
    "        # Do not show categories if xyticks is False\n",
    "        categories = False\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        cf,\n",
    "        annot=box_labels,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap,\n",
    "        cbar=cbar,\n",
    "        xticklabels=categories,\n",
    "        yticklabels=categories,\n",
    "    )\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\" + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "\n",
    "    if title:\n",
    "      plt.title(title)"
   ],
   "metadata": {
    "id": "WXSEBY_GJeZJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_preds(dataloader, model, device = 'cpu'):\n",
    "  \n",
    "  all_labels = torch.tensor([])\n",
    "  all_preds = torch.tensor([])\n",
    "\n",
    "  model.eval()\n",
    "  for i, batch in enumerate(dataloader):\n",
    "\n",
    "      input_ids = batch[0].to(device)\n",
    "      input_masks = batch[1].to(device)\n",
    "      labels = batch[2].to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        result = model(input_ids, token_type_ids = None, attention_mask = input_masks, labels = labels, return_dict = True)\n",
    "\n",
    "      loss = result.loss\n",
    "      logits = result.logits.detach().cpu()\n",
    "      labels = labels.cpu()\n",
    "\n",
    "      preds = torch.argmax(logits, axis = 1).flatten()\n",
    "      labels = labels.flatten()\n",
    "\n",
    "      all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "      all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "\n",
    "  return all_labels, all_preds"
   ],
   "metadata": {
    "id": "jDuE-OlmHDmq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_labels, all_preds = get_preds(train_dataloader, model, device)\n",
    "cm = confusion_matrix(all_labels, all_preds)"
   ],
   "metadata": {
    "id": "-sevG0kvlKOO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "make_confusion_heatmap(cm, figsize=(12,8))"
   ],
   "metadata": {
    "id": "UNKf-IkYKc7E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(all_labels, all_preds)"
   ],
   "metadata": {
    "id": "-wiYujTtK62l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is weird is that the model directly losses accuracy when we train it. It should gain in accuracy when we train it on a dowstream task. "
   ],
   "metadata": {
    "id": "6GshFvF1KF3S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The val loss is increasing way too much, it's overfitting. \n",
    "What we should do :\n",
    "\n",
    "- [x] Track the learning rate\n",
    "- [x] Call the scheduler after the validation step\n",
    "- [ ] Try different warmup steps\n",
    "- [x] Change the scheduler type\n",
    "- [x] Train on all data before submission"
   ],
   "metadata": {
    "id": "_RnyWaNdo6iJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a submission"
   ],
   "metadata": {
    "id": "B-nwOvnzkASO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_data = pd.read_csv(\"/content/drive/MyDrive/Machine Learning/Datasets/Disaster Tweets/test.csv\")"
   ],
   "metadata": {
    "id": "0dU62EQMkBS6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "tweets = test_data['text'].to_list()\n",
    "\n",
    "for tweet in tweets:\n",
    "  output_dict = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=90, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "  input_ids.append(output_dict.input_ids)\n",
    "  attention_masks.append(output_dict.attention_mask)\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ],
   "metadata": {
    "id": "IfNJEPVulsBk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "test_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)"
   ],
   "metadata": {
    "id": "52Qq2ORFl4KO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_preds = torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "\n",
    "  input_ids = batch[0].to(device)\n",
    "  input_masks = batch[1].to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    result = model(input_ids, token_type_ids = None, attention_mask = input_masks, return_dict = True)\n",
    "\n",
    "    logits = result.logits.detach().cpu()\n",
    "\n",
    "    preds = torch.argmax(logits, axis = 1).flatten()\n",
    "\n",
    "    all_preds = torch.cat((all_preds, preds), dim=0)"
   ],
   "metadata": {
    "id": "8-WxnowLmej7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ids = test_data['id']\n",
    "preds = pd.DataFrame(list(all_preds.type(torch.int)), columns = ['target'])\n",
    "res = pd.merge(ids, preds, left_index = True, right_index = True)"
   ],
   "metadata": {
    "id": "Tb9QLktXnudu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "res.to_csv(\"/content/drive/MyDrive/Machine Learning/Datasets/Disaster Tweets/res.csv\", sep = \",\", index = False)"
   ],
   "metadata": {
    "id": "NJmf2DAdovMG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis of tweets wrongly classified"
   ],
   "metadata": {
    "id": "qFDu93dGNBRl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "all_preds, all_labels = get_preds(train_dataloader, model, device)"
   ],
   "metadata": {
    "id": "Msd0cO41NAck"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wrong_preds = (all_preds != all_labels)"
   ],
   "metadata": {
    "id": "ipubwzzMO4Fi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wrong_preds = np.array(wrong_preds)"
   ],
   "metadata": {
    "id": "mCUH0mv4O_oI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wrong_tweets = data[wrong_preds]"
   ],
   "metadata": {
    "id": "yUaRWnSzPMbw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wrong_tweets['pred'] = (wrong_tweets['target'] + 1) % 2"
   ],
   "metadata": {
    "id": "-En4XJMwQIrg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wrong_tweets"
   ],
   "metadata": {
    "id": "4ThvBtsOQb8G"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Possible improvements"
   ],
   "metadata": {
    "id": "FaZevBtQRl1W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* To improve the model performance we could set a treshold that maximises the val accuracy rather than just using argmax, which is equivalent to threshold 50%\n",
    "* Create an ensemble learning architecture"
   ],
   "metadata": {
    "id": "ZNISlMTGRtUa"
   }
  }
 ]
}
